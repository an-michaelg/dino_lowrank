{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10706bbf-8b19-44e2-a050-426f45fd1b89",
   "metadata": {},
   "source": [
    "## Exploring segmentation capabilities of DINO features on ultrasound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a23f3-e89e-4abc-89a9-afc7105e7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('animation', html='jshtml')\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from model_lora_vit import get_vit, load_lora_vit_from_dino_ckpt\n",
    "from data_transforms import get_deterministic_transform\n",
    "from dataloader_tmed import TMED2\n",
    "\n",
    "torch.hub.set_dir(\"../pretrained_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11393397-f124-498e-a013-7f9041746f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the GPU\n",
    "device = 1 if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041e49d-1a9f-483d-8982-23f4b92ee7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the backbone model, ensure params are consistent with ckpt\n",
    "experiment = 'full'\n",
    "if experiment == 'imagenet':\n",
    "    ckpt_path = None\n",
    "    lora_rank = 0\n",
    "elif experiment == 'full':\n",
    "    ckpt_path = '../logs/training_base/checkpoint.pth'\n",
    "    lora_rank = 0\n",
    "elif experiment == 'lora4':\n",
    "    ckpt_path = '../logs/training_1/checkpoint0009.pth'\n",
    "    lora_rank = 4\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "arch = 'vit_small'\n",
    "patch_size = 8\n",
    "if ckpt_path == None:\n",
    "    # load the default DINO model\n",
    "    model = get_vit(arch, patch_size, lora_rank=0)\n",
    "else:\n",
    "    model = get_vit(arch, patch_size, lora_rank)\n",
    "    load_lora_vit_from_dino_ckpt(model, ckpt_path)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa52434-dbdf-4866-be4c-32e27f3e65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "transform = get_deterministic_transform()\n",
    "tr_dataset = TMED2(\n",
    "    split = \"train\", # train/val/test/all/unlabeled\n",
    "    transform = transform,\n",
    "    parasternal_only = True,\n",
    "    label_scheme_name = 'tufts',\n",
    ")\n",
    "tr_dataloader = torch.utils.data.DataLoader(tr_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "va_dataset = TMED2(\n",
    "    split = \"val\", # train/val/test/all/unlabeled\n",
    "    transform = transform,\n",
    "    parasternal_only = True,\n",
    "    label_scheme_name = 'tufts',\n",
    ")\n",
    "va_dataloader = torch.utils.data.DataLoader(va_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a3215-b36e-4941-8855-2e98be316125",
   "metadata": {},
   "source": [
    "### First take a look at the image pipeline \n",
    "DINO accepts BxCxHxW dimensionality. \n",
    "\n",
    "Images are normalized using ImageNet weights, which were used to train DINO.\n",
    "\n",
    "When visualizing the batch, we have to un-normalize the image and permute the image dimensions to HxWxC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a18fc6-0d51-45d0-bc1e-264e773ca07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(im):\n",
    "    # reverses Imagenet/DINO normalization\n",
    "    # assumes input size is 3xHxW\n",
    "    inv_normalize = Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255], std=[1/0.229, 1/0.224, 1/0.255], inplace=True)\n",
    "    return inv_normalize(im)\n",
    "\n",
    "# return unnormalized frames for viewing\n",
    "def unnormalize_batch_of_frames(imgs):\n",
    "    imgs_norm = imgs.detach().clone()\n",
    "    B = imgs_norm.shape[0]\n",
    "    for i in range(B):\n",
    "        imgs_norm[i] = unnormalize_frame(imgs_norm[i])\n",
    "    return imgs_norm\n",
    "\n",
    "def unnormalize_frame(img):\n",
    "    # reverses Imagenet/DINO normalization\n",
    "    # assumes input size is 3xHxW\n",
    "    inv_normalize = Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255], std=[1/0.229, 1/0.224, 1/0.255])\n",
    "    return inv_normalize(img)\n",
    "\n",
    "def show_batch_of_frames(imgs):\n",
    "    B, C, H, W = imgs.shape\n",
    "    fig, axs = plt.subplots(ncols=B, squeeze=False, figsize=(2*B,6))\n",
    "    for i in range(B):\n",
    "        img = imgs[i].detach().cpu().numpy().transpose(1,2,0)\n",
    "        axs[0, i].imshow(img)\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "# visualize one batch\n",
    "data_iter = iter(tr_dataloader)\n",
    "x, [y, y_view] = next(data_iter)\n",
    "print(\n",
    "        f\"target_AS: {y}\\n\"\n",
    "        f\"view: {y_view}\\n\"\n",
    "        f\"cine shape: {x.shape}\"\n",
    "    )\n",
    "\n",
    "vis_cine = unnormalize(x[0])\n",
    "plt.imshow(vis_cine.cpu().numpy().transpose(1,2,0))\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77bc49-c525-447d-b5aa-470a757c9d23",
   "metadata": {},
   "source": [
    "### Perform PCA on the DINOv2 features\n",
    "DINOv2 can produce patch features.\n",
    "\n",
    "One useful way to visualize these patch features is to first run dimensionality reduction to turn the 384-D features to 3-D, then visualize each dimension as a RGB color channel.\n",
    "\n",
    "This helps us humans see which patches are close to each other in feature space.\n",
    "\n",
    "Sufficiently large differences in patch features imply that one patch may contain semantically different information to another patch, this is an approximation of \"segmentation\", which applies well to natural images but not necessarily medical images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beb264-6f91-43cb-aeec-0b44483e6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the PCA of the patch embeddings, for one frame\n",
    "features = model.get_intermediate_layers(x.to(device))[0]\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d6882-ed8a-4c6e-af98-145d5b216530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of patches horizontally and vertically based on the return dimension\n",
    "H = int(np.sqrt(features.shape[1] - 1))\n",
    "assert H**2 == features.shape[1] - 1\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384c0e1-f66c-42cb-b9f9-f3b4f24326e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_image_pca(features, patch_dim=16):\n",
    "    pca_features, _ = single_image_pca(features)\n",
    "    # normalize to [0,1]\n",
    "    pca_features = (pca_features - pca_features.min()) / (pca_features.max() - pca_features.min())\n",
    "    pca_features = pca_features * 255\n",
    "    \n",
    "    plt.imshow(pca_features.reshape(patch_dim, patch_dim, 3).astype(np.uint8))\n",
    "    plt.show()\n",
    "\n",
    "def single_image_pca(features):\n",
    "    # return the PCA'd features and the PCA object for more .transform() calls\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(features)\n",
    "    \n",
    "    pca_features = pca.transform(features)\n",
    "    return pca_features, pca\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    img_features = features[i].detach().cpu().numpy() # 785 x 384\n",
    "    patch_features = img_features[1:, :]  # 784 x 384\n",
    "    plot_single_image_pca(patch_features, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e8fbc-937f-4d01-888c-abeafa0f03da",
   "metadata": {},
   "source": [
    "### Background removal\n",
    "By looking at the value of the first component, we can separate the \"foreground\" of the image and the \"background\" of the image. \n",
    "\n",
    "This is not always consistent but can generally segment the beam area.\n",
    "\n",
    "We can subsequently run a PCA on only the foreground features to visualize the similarity between foreground patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e49a9-7dee-4cc0-aa78-2649813c3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background removal on the PCA of one image based on the first principal component\n",
    "def single_image_background(features, threshold=0):\n",
    "    # patch features are H**2 x 3 after PCA\n",
    "    pca_features, _ = single_image_pca(features)\n",
    "    background = pca_features[:,0] <= 0 # H**2\n",
    "    return background\n",
    "    \n",
    "#patch_features = features['x_norm_patchtokens'][0].detach().cpu().numpy()\n",
    "for i in range(x.shape[0]):\n",
    "    patch_features = features[i, 1:, :].detach().cpu().numpy()\n",
    "    plt.imshow(single_image_background(patch_features).reshape(H, H))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b43277-a1cb-478b-9578-3a1a6a0ed245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to summarize what we did:\n",
    "# remove the background from each image, compute the PCA of the leftover components altogether\n",
    "def pca_based_part_segmentation(imgs, learned_pca_obj=None, animate=False):\n",
    "    # assume imgs are BxCxHxW tensors\n",
    "    x = imgs.to(device)\n",
    "    B = x.shape[0]\n",
    "    features = model.get_intermediate_layers(x.to(device))[0] # Bx785x384\n",
    "    features = features.detach().cpu().numpy()\n",
    "    patch_features = features[:, 1:, :]\n",
    "\n",
    "    # calculate the number of patches\n",
    "    H = int(np.sqrt(patch_features.shape[1]))\n",
    "    assert H**2 == patch_features.shape[1]\n",
    "\n",
    "    # extract the background\n",
    "    background = []\n",
    "    for i in range(B):\n",
    "        background.append(single_image_background(patch_features[i]))\n",
    "        \n",
    "    # keep track of the positions where the image is not the background\n",
    "    positions = []\n",
    "    non_background_features = []\n",
    "    for bg in background:\n",
    "        positions.append(np.argwhere(bg == False)[:,0])\n",
    "        \n",
    "    # compound the features from the non-background pixels\n",
    "    for i in range(B):\n",
    "        pos_indices = positions[i]\n",
    "        for p in pos_indices:\n",
    "            non_background_features.append(features[i][p])\n",
    "    non_background_features = np.array(non_background_features) # N'x384 where N' < B*(H**2)\n",
    "\n",
    "    if learned_pca_obj is None:\n",
    "        # we run PCA to learn these features\n",
    "        nb_pca_features, pca_obj = single_image_pca(non_background_features) # N'x3\n",
    "    else:\n",
    "        # we use PCA.transform with the learned PCA\n",
    "        nb_pca_features = learned_pca_obj.transform(non_background_features)\n",
    "    # re-center these features\n",
    "    nb_pca_features = (nb_pca_features - nb_pca_features.min()) / (nb_pca_features.max() - nb_pca_features.min())\n",
    "    nb_pca_features = nb_pca_features * 255\n",
    "\n",
    "    # re-integrate these features into the image\n",
    "    new_images = np.zeros((B, H**2, 3))\n",
    "    j = 0\n",
    "    for i in range(B):\n",
    "        pos_indices = positions[i]\n",
    "        for p in pos_indices:\n",
    "            new_images[i,p,:] = nb_pca_features[j, :]\n",
    "            j += 1\n",
    "\n",
    "    # show the final results\n",
    "    ret = {}\n",
    "    vis_imgs = np.clip(unnormalize_batch_of_frames(imgs).numpy(), 0, 1)\n",
    "    if animate:\n",
    "        frames = []\n",
    "        fig, axs = plt.subplots(ncols=2, squeeze=False, figsize=(6, 4))\n",
    "        for i in range(B):\n",
    "            left = axs[0,0].imshow(new_images[i].reshape(H, H, 3).astype(np.uint8))\n",
    "            right = axs[0,1].imshow(vis_imgs[i].transpose(1,2,0))\n",
    "            frames.append([left, right])\n",
    "            #print(frames[i])\n",
    "        ani = animation.ArtistAnimation(fig, frames)\n",
    "        ret['animation'] = ani\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        for i in range(B):\n",
    "            fig, axs = plt.subplots(ncols=2, squeeze=False, figsize=(6, 12))\n",
    "            axs[0,0].imshow(new_images[i].reshape(H, H, 3).astype(np.uint8))\n",
    "            axs[0,1].imshow(vis_imgs[i].transpose(1,2,0))\n",
    "            plt.show()\n",
    "        \n",
    "    # return the learned PCA to transfer over to video\n",
    "    if learned_pca_obj is None:\n",
    "        ret['learned_pca_obj'] = pca_obj\n",
    "    return ret\n",
    "\n",
    "learned_pca_obj = pca_based_part_segmentation(x)['learned_pca_obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f9985-fa6f-4a27-9985-d7ef0d19c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use the PCA features for video\n",
    "# def save_gif(name, ani):\n",
    "#     # save the gif if needed\n",
    "#     writergif = animation.PillowWriter(fps=3)\n",
    "#     ani.save(name +'.gif',writer=writergif) \n",
    "#     print(\"GIF saved at \" + name + \".gif\")\n",
    "\n",
    "# for i in range(len(sample_dict['cine'])):\n",
    "#     video = sample_dict['cine'][i, :, :, :, :] # BxCxTxHxW\n",
    "#     video = video.permute(1, 0, 2, 3)\n",
    "#     ani = pca_based_part_segmentation(video, learned_pca_obj, animate=True)['animation']\n",
    "#     #save_gif(str(i), ani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ea453-1855-4acd-9b40-b76b33ddfae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
